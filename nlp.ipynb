{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNu7ie1KNenGZ9op76UW30a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zZilman/---/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyAinULUGGvw",
        "outputId": "d75d4c4e-8b7e-4f73-973f-7b4d460c3a31"
      },
      "source": [
        "import re # Импортируем модуль для работы с регулярными выражениями\n",
        "\n",
        "test_str = '<p>Hello <b>World!!!</b><p>'\n",
        "print(test_str)\n",
        "\n",
        "# Используем функцию re.sub. Первый ее аргумент - шаблон строки, по которому нужно искать подстроки.\n",
        "# Второй аргумент - строка, на которую нужно заменить найденную подстроку. Третий аргумент - строка, \n",
        "# в которой нужно искать и заменять подстроки.\n",
        "clear_str = re.sub(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', test_str)\n",
        "print(clear_str)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<p>Hello <b>World!!!</b><p>\n",
            "Hello World!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S_jRYsCeEJq"
      },
      "source": [
        "import stanza # Импортируем библиотеку stanza\n",
        "# Чтобы использовать библиотеку для английского языка, нужно скачать набор моделей.\n",
        "stanza.download('en')\n",
        "\n",
        "test_str = '''Stanza is a Python natural language analysis package. \n",
        "It contains tools, which can be used in a pipeline, to convert a string containing human language \n",
        "text into lists of sentences and words, to generate base forms of those words, \n",
        "their parts of speech and morphological features, to give a syntactic structure dependency parse, \n",
        "and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, \n",
        "using the Universal Dependencies formalism.'''\n",
        "print('Исходный текст: {}'.format(test_str))\n",
        "\n",
        "\n",
        "# Создадим обработчик для английского языка, который будет включать в себя токенизатор\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize')\n",
        "\n",
        "# Передаем в созданный обработчик заданный текст и получаем на выходе обработанный текст, \n",
        "# записанный в определенной структуре.\n",
        "doc = nlp(test_str)\n",
        "\n",
        "tokens = [] # Создадим список, в котором будем хранить отдельные слова\n",
        "for sent in doc.sentences: # Перебираем предложения\n",
        "    for word in sent.words: # Перебираем все слова в каждом из предложений\n",
        "        tokens.append(word.text) # Чтобы получить доступ к слову, необходимо обратиться к нему word.text\n",
        "\n",
        "tokenized_str = ' '.join(tokens) # Объединяем отдельные слова в один текст, разделяя слова пробелами\n",
        "print('Токенизированный текст: {}'.format(tokenized_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwq2Cl66fiFv"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(test_str)\n",
        "\n",
        "tokenized_str = ' '.join(tokens)\n",
        "print('Токенизированный текст: {}'.format(tokenized_str))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St1cQfBbgxtI"
      },
      "source": [
        "test_str = '''Stanza is a Python natural language analysis package. \n",
        "It contains tools, which can be used in a pipeline, to convert a string containing human language \n",
        "text into lists of sentences and words, to generate base forms of those words, \n",
        "their parts of speech and morphological features, to give a syntactic structure dependency parse, \n",
        "and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, \n",
        "using the Universal Dependencies formalism.'''\n",
        "print('Исходный текст: {}'.format(test_str))\n",
        "\n",
        "doc = nlp(test_str)\n",
        "\n",
        "tokens = []\n",
        "for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "        tokens.append(word.text.lower()) # Приведение к нижнему регистру\n",
        "\n",
        "tokenized_str = ' '.join(tokens)\n",
        "print('Токенизированный текст: {}'.format(tokenized_str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOSBFVwmhi4P"
      },
      "source": [
        "test_str = '''Stanza is a Python natural language analysis package. \n",
        "It contains tools, which can be used in a pipeline, to convert a string containing human language \n",
        "text into lists of sentences and words, to generate base forms of those words, \n",
        "their parts of speech and morphological features, to give a syntactic structure dependency parse, \n",
        "and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, \n",
        "using the Universal Dependencies formalism.'''\n",
        "print('Исходный текст: {}'.format(test_str))\n",
        "\n",
        "# Помимо токенизатора используем модули, отвечающие за определение части речи. \n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')\n",
        "\n",
        "doc = nlp(test_str)\n",
        "\n",
        "tokens = []\n",
        "for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "        # Все числа имеюбт часть речи NUM\n",
        "        # А если вы захотите удалить все знаки пунктуации, то вам поможет часть речи PUNCT\n",
        "        if word.upos == 'NUM':\n",
        "            tokens.append('__NUM__') # Заменяем все числа на специальный токен\n",
        "        else:\n",
        "            tokens.append(word.text.lower()) # Если нам встретилось не число, то помещаем его в список\n",
        "\n",
        "tokenized_str = ' '.join(tokens)\n",
        "print('Токенизированный текст без чисел: {}'.format(tokenized_str))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPPbct5Ah-_w"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "swords = stopwords.words('english')\n",
        "print(swords)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm616nJxiCYO"
      },
      "source": [
        "Рассмотрим, как удалить стоп-слова при обработке текста библиотекой stanza и при разделении текста с помощью функции split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZZjfKwEiJKF"
      },
      "source": [
        "test_str = '''Stanza is a Python natural language analysis package. \n",
        "It contains tools, which can be used in a pipeline, to convert a string containing human language \n",
        "text into lists of sentences and words, to generate base forms of those words, \n",
        "their parts of speech and morphological features, to give a syntactic structure dependency parse, \n",
        "and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, \n",
        "using the Universal Dependencies formalism.'''\n",
        "print('Исходный текст: {}'.format(test_str))\n",
        "\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos')\n",
        "\n",
        "doc = nlp(test_str)\n",
        "\n",
        "tokens = []\n",
        "for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "        if word.upos == 'NUM':\n",
        "            tokens.append('__NUM__')\n",
        "        else:\n",
        "            token = word.text.lower()\n",
        "            if token not in swords:\n",
        "                tokens.append(token)\n",
        "\n",
        "tokenized_str = ' '.join(tokens)\n",
        "print('Токенизированный текст без чисел: {}'.format(tokenized_str))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rospyvcljBR2"
      },
      "source": [
        "Рассмотрим, как можно реализовать лематизацию при помощи библиотеки stanza."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVOzUTlCi1P-"
      },
      "source": [
        "est_str = '''Stanza is a Python natural language analysis package. \n",
        "It contains tools, which can be used in a pipeline, to convert a string containing human language \n",
        "text into lists of sentences and words, to generate base forms of those words, \n",
        "their parts of speech and morphological features, to give a syntactic structure dependency parse, \n",
        "and to recognize named entities. The toolkit is designed to be parallel among more than 60 languages, \n",
        "using the Universal Dependencies formalism.'''\n",
        "print('Исходный текст: {}'.format(test_str))\n",
        "\n",
        "# Добавляем обработчик, ответственный за нахождение изначальных форм слов\n",
        "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
        "\n",
        "doc = nlp(test_str)\n",
        "\n",
        "tokens = []\n",
        "for sent in doc.sentences:\n",
        "    for word in sent.words:\n",
        "        if word.upos == 'NUM':\n",
        "            tokens.append('__NUM__')\n",
        "        else:\n",
        "            token = word.lemma.lower() # Находим изначальную форму слова\n",
        "            if token not in swords:\n",
        "                tokens.append(token)\n",
        "\n",
        "tokenized_str = ' '.join(tokens)\n",
        "print('Токенизированный текст без чисел: {}'.format(tokenized_str))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}